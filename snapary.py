import streamlit as st
from pytubefix import YouTube
from urllib.parse import urlparse, parse_qs
from moviepy.editor import VideoFileClip
from youtube_transcript_api import YouTubeTranscriptApi
import whisper
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector
import cv2
import os
import pandas as pd
from openai import OpenAI
import time

st.set_page_config(page_title="YouTube Video Processing", layout="wide")

# ----------------------------
# Helper functions (from your code)
# ----------------------------

def get_video_id(youtube_url):
    query = urlparse(youtube_url)
    if query.hostname == 'youtu.be':
        return query.path[1:]
    if query.hostname in ('www.youtube.com', 'youtube.com'):
        if query.path == '/watch':
            return parse_qs(query.query)['v'][0]
        elif query.path.startswith('/embed/'):
            return query.path.split('/')[2]
        elif query.path.startswith('/v/'):
            return query.path.split('/')[2]
    raise ValueError("ì˜¬ë°”ë¥¸ YouTube ë§í¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")

def download_youtube_video(url, download_audio_only=False):
    try:
        yt = YouTube(url)
        st.info(f"ğŸ¥ ì˜ìƒ ì œëª©: {yt.title}")
        if download_audio_only:
            stream = yt.streams.filter(only_audio=True).first()
            st.info("ğŸ§ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        else:
            stream = yt.streams.get_highest_resolution()
            st.info("ğŸ“¥ ì˜ìƒ ìŠ¤íŠ¸ë¦¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        output_path = stream.download()
        st.success(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_path}")
        return output_path
    except Exception as e:
        st.error(f"âš ï¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def extract_audio_from_video(video_path, audio_path="extracted_audio.wav"):
    video = VideoFileClip(video_path)
    video.audio.write_audiofile(audio_path, codec='pcm_s16le')
    return audio_path

def get_captions(video_url, lang='en'):
    video_id = get_video_id(video_url)
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[lang])
        st.success("âœ… ìë§‰ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        return transcript
    except Exception as e:
        st.warning(f"âŒ ìë§‰ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return None

def transcribe_audio_whisper(audio_path, model_size):
    model = whisper.load_model(model_size)
    result = model.transcribe(audio_path)
    return result

def convert_whisper_to_caption_format(result):
    segments = result.get('segments', [])
    captions = []
    for segment in segments:
        text = segment['text'].strip()
        start = segment['start']
        end = segment['end']
        duration = round(end - start, 2)
        captions.append({'text': text, 'start': round(start, 2), 'duration': duration})
    return captions

def extract_txt(video_url, cap_lang, video_path=None, model_size="base"):
    captions = get_captions(video_url, cap_lang)
    if not captions:
        if video_path is None:
            st.error("video_path is í•„ìš”í•©ë‹ˆë‹¤. ìë§‰ ì—†ì„ ë•Œ.")
            return None
        audio_path = extract_audio_from_video(video_path)
        result = transcribe_audio_whisper(audio_path, model_size=model_size)
        captions = convert_whisper_to_caption_format(result)
    return captions

def detect_and_save_scenes(video_path, threshold, output_dir="scene_images"):
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    scene_manager.add_detector(ContentDetector(threshold=threshold))
    video_manager.set_downscale_factor()
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)
    scene_list = scene_manager.get_scene_list()
    video_manager.release()
    os.makedirs(output_dir, exist_ok=True)
    cap = cv2.VideoCapture(video_path)
    scene_info = []
    for i, scene in enumerate(scene_list):
        start_time = scene[0].get_seconds()
        end_time = scene[1].get_seconds()
        mid_time = (start_time + end_time) / 2
        cap.set(cv2.CAP_PROP_POS_MSEC, mid_time * 1000)
        ret, frame = cap.read()
        if ret:
            filename = f'scene_{i+1:03}.jpg'
            filepath = os.path.join(output_dir, filename)
            cv2.imwrite(filepath, frame)
            scene_info.append({
                "scene": i+1,
                "start_time": start_time,
                "end_time": end_time,
                "image_path": filepath
            })
    cap.release()
    return scene_info

def split_into_chapters_df(captions, api_key, model="gpt-3.5-turbo"):
    client = OpenAI(api_key=api_key)
    text_with_time = []
    for cap in captions:
        end = cap['start'] + cap['duration']
        text_with_time.append(f"[{cap['start']:.2f} ~ {end:.2f}] {cap['text'].strip()}")
    joined_text = "\n".join(text_with_time)
    prompt = f"""
ë‹¤ìŒì€ ì˜ìƒ ìë§‰ì…ë‹ˆë‹¤. ì‹œê°„ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ì£¼ìš” ë‚´ìš©ì„ ê¸°ì¤€ìœ¼ë¡œ ë…¼ë¦¬ì ì¸ ì±•í„°ë¡œ ë‚˜ëˆ„ê³ ,
ê° ì±•í„°ë§ˆë‹¤ ë‹¤ìŒ í˜•ì‹ì— ë§ê²Œ ì¶œë ¥í•´ì£¼ì„¸ìš”:

ì œëª© | ì‹œì‘ì‹œê°„ | ì¢…ë£Œì‹œê°„

ê° í•­ëª©ì€ ë°˜ë“œì‹œ '|' ê¸°í˜¸ë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš”.
ì‹œê°„ì€ ì´ˆ ë‹¨ìœ„ ìˆ«ì(float)ë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
ëª¨ë¸ ì†Œê°œ | 0.0 | 45.2
í›ˆë ¨ ë°ì´í„° | 45.2 | 89.4

ìë§‰:
{joined_text}
"""
    time.sleep(1)
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5
    )
    result = response.choices[0].message.content.strip()
    rows = []
    for line in result.split('\n'):
        line = line.strip()
        if not line or '|' not in line:
            continue
        if 'ì œëª©' in line or 'ì‹œì‘ì‹œê°„' in line:
            continue
        try:
            title, start, end = [part.strip() for part in line.split('|')]
            rows.append({"chapter_title": title, "start_time": float(start), "end_time": float(end)})
        except:
            continue
    df = pd.DataFrame(rows)
    return df

def summarize_chapters(captions, chapter_df, api_key, model):
    client = OpenAI(api_key=api_key)
    summaries = []
    for _, row in chapter_df.iterrows():
        start, end = row['start_time'], row['end_time']
        segment_texts = [cap['text'].strip() for cap in captions if start <= cap['start'] <= end]
        joined_text = " ".join(segment_texts)
        prompt = f"""
ë‹¤ìŒì€ í•˜ë‚˜ì˜ ì˜ìƒ ì±•í„°ì— í•´ë‹¹í•˜ëŠ” ìë§‰ì…ë‹ˆë‹¤
ì´ ìë§‰ì„ ë°”íƒ•ìœ¼ë¡œ ì¤‘ìš”í•œ ë‚´ìš©ì„ ì¶”ë ¤, ChatGPTê°€ ì‘ì„±í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ê¸° ì¢‹ê³  ì½ê¸° ì‰½ê²Œ ì •ë¦¬í•´ ì£¼ì„¸ìš”:
    - ë¬¸ë‹¨ ëŒ€ì‹  ë“¤ì—¬ì“°ê¸°, ë¶ˆë¦¿, ì¤„ ë°”ê¿ˆ, ì´ëª¨ì§€ í™œìš©
    - í•µì‹¬ ê°œë…ê³¼ í‚¤ì›Œë“œ ê°•ì¡°, í•„ìš”ì‹œ ê°„ë‹¨ ì„¤ëª… ì¶”ê°€
    - ì˜ìƒ íë¦„ê³¼ ì˜ë„ ì˜ ë“œëŸ¬ë‚˜ë„ë¡ ìì„¸íˆ ì‘ì„±
    - ëª…ì‚¬í˜• ì¢…ê²°ì–´ë¯¸ ì‚¬ìš©
    - í•œêµ­ì–´ë¡œ ì‘ì„±

ìë§‰:
{joined_text}
""".strip()
        time.sleep(1)
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5
            )
            summary = response.choices[0].message.content.strip()
        except Exception as e:
            st.warning(f"ìš”ì•½ ì‹¤íŒ¨: {row['chapter_title']} ({e})")
            summary = ""
        summaries.append(summary)
    chapter_df['chapter_summary'] = summaries
    return chapter_df

# ----------------------------
# Streamlit UI
# ----------------------------

st.title("ğŸ¬ Snapary")

api_key = st.text_input("ğŸ”‘ OpenAI API í‚¤ ì…ë ¥", type="password")
video_url = st.text_input("ğŸ“º YouTube ì˜ìƒ URL ì…ë ¥")

download_audio_only = st.checkbox("ì˜¤ë””ì˜¤ë§Œ ë‹¤ìš´ë¡œë“œ (ìŒì„± ì¶”ì¶œìš©)", value=False)
caption_lang = st.selectbox("ìë§‰ ì–¸ì–´ ì„ íƒ", options=["en", "ko", "ja", "es", "fr"], index=0)
whisper_model_size = st.selectbox("Whisper ëª¨ë¸ í¬ê¸° ì„ íƒ", options=["tiny", "base", "small", "medium", "large"], index=1)
scene_threshold = st.slider("ì¥ë©´ ì „í™˜ ê°ì§€ ì„ê³„ê°’ (0~30)", 0.0, 30.0, 27.0)

if st.button("ì˜ìƒ ì²˜ë¦¬ ì‹œì‘"):
    if not video_url or not api_key:
        st.error("YouTube URLê³¼ OpenAI API í‚¤ë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”!")
    else:
        with st.spinner("ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘..."):
            video_path = download_youtube_video(video_url, download_audio_only=download_audio_only)
        if video_path is None:
            st.error("ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨!")
        else:
            with st.spinner("ìë§‰ ë¶ˆëŸ¬ì˜¤ê¸° ë˜ëŠ” Whisper ìŒì„± ì¸ì‹ ì¤‘..."):
                captions = extract_txt(video_url, caption_lang, video_path=video_path, model_size=whisper_model_size)
            if not captions:
                st.error("ìë§‰/ìŒì„± ì¸ì‹ ì‹¤íŒ¨!")
            else:
                st.success(f"ìë§‰ {len(captions)}ê°œ ë¬¸ì¥ ë¶ˆëŸ¬ì˜´")

                with st.spinner("ì¥ë©´ ê°ì§€ ë° ì´ë¯¸ì§€ ì €ì¥ ì¤‘..."):
                    scenes = detect_and_save_scenes(video_path, scene_threshold)
                    st.success(f"ì¥ë©´ {len(scenes)}ê°œ ê°ì§€ë¨")

                st.write("### ì¥ë©´ ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸°")
                cols = st.columns(min(len(scenes), 4))
                for i, scene in enumerate(scenes):
                    with cols[i % 4]:
                        st.image(scene['image_path'], caption=f"Scene {scene['scene']} [{scene['start_time']:.2f}s~{scene['end_time']:.2f}s]")

                with st.spinner("ì±•í„° ë¶„í•  ì¤‘..."):
                    chapters_df = split_into_chapters_df(captions, api_key, model="gpt-3.5-turbo")
                    st.dataframe(chapters_df)

                with st.spinner("ì±•í„°ë³„ ìš”ì•½ ìƒì„± ì¤‘..."):
                    chapters_df = summarize_chapters(captions, chapters_df, api_key, model="gpt-3.5-turbo")
                    st.dataframe(chapters_df[['chapter_title', 'chapter_summary']])

                st.balloons()